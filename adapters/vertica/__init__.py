import sys
import os
import vertica_python
import json
from log import Log
from .converter import convert_data
from datetime import datetime
from util import Util
from metadata import MetaData
from constants import Constants
from config import ConfigMap


class VerticaConnection:
    def __init__(self, host, port, user, password, dbname, schema, config):
        conn_info = {
            "host": host,
            "port": port,
            "user": user,
            "password": password,
            "database": dbname,
            # autogenerated session label by default,
            # "session_label": "some_label",
            # default throw error on invalid UTF-8 results
            "unicode_error": "strict",
            # SSL is disabled by default
            "ssl": False,
            # autocommit is off by default
            "autocommit": True,
            # using server-side prepared statements is disabled by default
            "use_prepared_statements": False,
            # connection timeout is not enabled by default
            # 5 seconds timeout for a socket operation (Establishing a TCP connection or read/write operation)
            "connection_timeout": 5,
        }
        self.connection = vertica_python.connect(**conn_info)
        self.cur = self.connection.cursor()
        self.schema = schema
        self.bucket_size = 10
        self.current_bucket = []
        self.global_config = config
        self.insert_row_count = 0

    def close_connection(self):
        self.connection.close()

    def __get_mapped_data(self, val):
        # TODO: Not a suitable way. use has_key instead
        try:
            return ConfigMap.config()[val]
        except:
            return val

    def __get_mapped_data_type(self, val, as_target=False):
        # TODO: Not a suitable way. use has_key instead
        try:
            for dt in ConfigMap.config()["datatypes"]:
                if as_target:
                    if dt["target"] == val:
                        return dt["source"]
                else:
                    if dt["source"] == val:
                        return dt["target"]
        except:
            return val

    def get_table_schema(
        self, tablename, table_config={"columns": []}, as_target=False
    ):
        query = f"""select column_name, is_nullable, data_type, data_type_length from v_catalog.columns where table_name ='{tablename}' AND table_schema='{self.schema}'"""
        if len(table_config["columns"]) > 0:
            columns = ", ".join("'" + item + "'" for item in table_config["columns"])
            query = f"{query} AND column_name in ({columns})"
        self.cur.execute(query)
        query_results = self.cur.fetchall()
        final_result = []
        for qr in query_results:
            final_result.append(
                {
                    "name": self.__get_mapped_data(qr[0]),
                    "nullable": self.__get_mapped_data(qr[1]),
                    "type": self.__get_mapped_data_type(qr[2], as_target),
                    "length": self.__get_mapped_data(qr[3]),
                }
            )
        return final_result

    def create_table(self, table_object, ignore_ts_fields=False):
        str_columns = ""
        table_name = table_object["name"]
        abs_table_name = Util.get_abs_table_name(table_name)
        Log.info(
            f"Creating table: {json.dumps(table_object, indent=4, sort_keys=True) }",
            table_name,
        )
        for col in table_object["s_columns"]:

            str_columns = f"{str_columns} {col['name']} {self.__get_mapped_data_type(col['type'])}"
            if "length" in col and col["length"]:
                str_columns = f"{str_columns}({col['length']})"
            if "nullable" in col and col["nullable"] == False:
                str_columns = f"{str_columns} NOT NULL"
            str_columns = f"{str_columns},"
        if ignore_ts_fields:
            str_columns = str_columns[:-1]
        else:
            str_columns = (
                f"{str_columns} {Constants.collection_time_stamp_column_name} INTEGER,"
            )
            str_columns = (
                f"{str_columns} {Constants.modification_time_stamp_column_name} INTEGER"
            )
        query = f"""CREATE TABLE IF NOT EXISTS {self.schema}.{abs_table_name} ({str_columns})"""
        Log.info(f"Executing query : {query}", table_name)
        self.cur.execute(query)

    def insert_values(self, table, values, ignore_ts_fields=False):
        Log.info("Row add/update request received")
        table_name = table["name"]
        query = self.get_query_to_insert(values, table, ignore_ts_fields)
        Log.debug(query, table_name)
        self.current_bucket.append(query)
        if len(self.current_bucket) == self.bucket_size:
            for q in self.current_bucket:
                if q != None:
                    self.execute(q)
            self.current_bucket = []

    def insert_bulk_values(self, table, values, ignore_ts_fields=False):
        table_name = table["name"]
        self.insert_row_count = 0
        for value in values:
            query = self.get_query_to_insert(value, table, ignore_ts_fields)
            Log.debug(query, table_name)
            if query != "" and query != None and len(query) > 10:
                self.insert_row_count = self.insert_row_count + 1
                Log.debug(f"Insert query: {query}", table_name)
                self.cur.execute(query)

    def finalize_insert(self):
        Log.info(f"Received finalization insertion")
        for q in self.current_bucket:
            if q != None:
                self.execute(q)
            self.current_bucket = []

    def execute(self, query):
        Log.info(f"Executing: {query}")
        self.cur.execute(query)
        query_results = self.cur.fetchall()
        return query_results

    def execute_select_query(self, columns, table_name, suffix=""):
        query = f"""SELECT {columns} FROM {self.schema}.{table_name} {suffix}"""
        return self.execute(query)

    @staticmethod
    def ticks(dt):
        return (dt - datetime(1, 1, 1)).total_seconds() * 10000000

    def get_query_to_insert(self, value, table, ignore_ts_fields=False):
        table_name = table["name"]
        abs_table_name = Util.get_abs_table_name(table_name)
        # Preparing all columns list
        columns = ""
        if len(table["s_columns"]) > 0:
            col_collection = []
            for col in table["s_columns"]:
                col_collection.append(col["name"])
            columns = ", ".join(col_collection)
            if ignore_ts_fields == False:
                columns = f"{columns}, {Constants.collection_time_stamp_column_name} , {Constants.modification_time_stamp_column_name}"
        else:
            columns = "*"

        # Preparging insert column list
        col_values = []
        unique_columns = table["uniqucolumns"] if "uniqucolumns" in table else []
        unique_columns_values = ""
        col_index = 0
        for col in table["s_columns"]:
            converted_value = convert_data(value[col_index], col["type"])
            col_values.append(converted_value)
            if col["name"] in unique_columns:
                if unique_columns_values != "":
                    unique_columns_values = f"{unique_columns_values} AND "
                unique_columns_values = (
                    f"{unique_columns_values} {col['name']}={converted_value}"
                )
            col_index = col_index + 1
        final_value = ""

        # Checking if the row exists
        check_query = f"""SELECT {columns} FROM {self.schema}.{abs_table_name} WHERE {unique_columns_values} LIMIT 1"""
        Log.debug(f"check_query: {check_query}")

        self.cur.execute(check_query)
        query_result = self.cur.fetchall()

        if len(query_result) > 0:
            # Row exist. Now need to check if its an exact copy
            data = query_result[0]
            data_index = 0
            match = True
            for d in data:
                if (
                    match
                    and str(col_values[data_index]) != str(d)
                    and data_index < len(data) - 1
                ):
                    match = False
                data_index = data_index + 1

            if match:
                # Its an exact match: IGNORE
                return ""
            else:
                col_update_values = ""
                col_update_index = 0
                for col in table["s_columns"]:
                    if col["name"] not in unique_columns:
                        converted_value = convert_data(
                            value[col_update_index], col["type"]
                        )
                        col_update_values = (
                            f"{col_update_values} {col['name']}={converted_value},"
                        )
                    col_update_index = col_update_index + 1
                col_update_values = col_update_values[:-1]
                if ignore_ts_fields == False:
                    col_update_values = f"{col_update_values},{Constants.modification_time_stamp_column_name}={VerticaConnection.ticks(datetime.utcnow())}"
                return f"""UPDATE {self.schema}.{abs_table_name} SET {col_update_values} WHERE {unique_columns_values}"""
        else:
            # Its a new row
            for val in col_values:
                if final_value != "":
                    final_value = f"{final_value},"
                final_value = f"{final_value} {val}"
            if ignore_ts_fields == False:
                final_value = f"{final_value}, {VerticaConnection.ticks(datetime.utcnow())}, {VerticaConnection.ticks(datetime.utcnow())}"
            return (
                f"""INSERT INTO {self.schema}.{abs_table_name} VALUES ({final_value})"""
            )
